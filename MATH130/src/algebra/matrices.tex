%-----------------------------------------------------------------------------%
%- Algebra :: Matrices -------------------------------------------------------%
%-----------------------------------------------------------------------------%
\chapter{TODO: Matrices}
\label{chap:Matrices}

\begin{align}
  \intertext{Addition}
  \mathbb{Z}_{M,N} & = \mathbb{A} + \mathbb{B} \nonumber \\
         {z}_{i,j} & = {a}_{i,j} + {b}_{i,j} \\
  \intertext{Subtraction}
  \mathbb{Z}_{M,N} & = \mathbb{A} - \mathbb{B} \nonumber \\
         {z}_{i,j} & = {a}_{i,j} - {b}_{i,j} \\
  \intertext{Multiplication}
  \mathbb{Z}_{M,N} & = \mathbb{A} \times \mathbb{B} \nonumber \\
         {z}_{i,j} & = \sum_{k=1}^{N} {a}_{i,k} \times {b}_{k,j}
\end{align}

\section{Language of Matrices}
\label{sec:LanguageOfMatrices}
A matrix is nothing more than a bunch of numbers arranged into a grid of
\emph{M} rows and \emph{N} columns and is represented usually by $\mathbb{A}$.
To represent a specific element inside the matrix $\mathbb{A}$, it is usually
referred to as $a_{i,j}$ \footnote{I don't know why we change to lowercase, but
it is the convention. Maybe it is to prevent confusion between referring to a
whole matrix (of particular dimensions) and a specific element of a matrix.}

An example of a ``3 by 4'' matrix called $\mathbb{A}$ is as follows:
\begin{align}
\mathbb{A}_{2,3} & = 
\begin{bmatrix}
  a_{1,1}   &   a_{1,2}   &   a_{1,3}   &   a_{1,4}   \\
  a_{2,1}   &   a_{2,2}   &   a_{2,3}   &   a_{2,4}   \\
  a_{3,1}   &   a_{3,2}   &   a_{3,3}   &   a_{3,4}   \\
\end{bmatrix}
\end{align}
Or more generally:
\begin{align}
\mathbb{A}_{M,N} & = 
\begin{bmatrix}
  a_{1,1}   &   a_{1,2}   &   a_{1,3}   &   \ldots   &   a_{1,j}   \\
  a_{2,1}   &   a_{2,2}   &   a_{2,3}   &   \ldots   &   a_{2,j}   \\
  \vdots    &   \vdots    &   \vdots    &   \ddots   &   \vdots    \\
  a_{i,1}   &   a_{i,2}   &   a_{i,3}   &   \ldots   &   a_{i,j}   \\
\end{bmatrix}
\label{eq:FormOfMatrices}
\end{align}
\\
\emph{In matrix algebra we go vertically describing the rows first then
horizontally we describe the columns.}\footnote{This is in contrast to the
cartesian plane e.g. map reading where we go horizontally then vertically.}

There is also a special kind of matrix called the \emph{identity matrix} often
represented by $\mathbb{I}$. This is a special matrix, which must be square in
shape (ie for $\mathbb{A}_{M,N}, M = N$), and consists of all zeroes except for
a diagonal row of ones from the top left corner to the bottom right corner.

A 4x4 identity matrix:
\begin{align}
  \mathbb{I}_{4,4}~&=
  \begin{bmatrix}
    ~1~&~0~&~0~&~0~\\
    ~0~&~1~&~0~&~0~\\
    ~0~&~0~&~1~&~0~\\
    ~0~&~0~&~0~&~1~\\
  \end{bmatrix}
  \label{eq:4x4IdentityMatrix}
\end{align}

An important property of the identity matrix is that when a matrix is multiplied
by it's identity matrix, the product is the original matrix.
\begin{align}
  \mathbb{A}~\times~\mathbb{I}~&=~\mathbb{A}
\end{align}
Another impotant property is that a matrix can be multiplied by it's identity
matrix in either order (we will see later in section
\ref{sec:MultiplyingMatrices}, Multiplying Matrices that in matrix
algebra, order is important\footnote{as opposed to normal algebra where you can
multiply in any which way and it doesn't matter}.
\begin{align}
  \mathbb{I}~\times~\mathbb{A}~&=~\mathbb{A}
\end{align}

\section{Adding Matrices}
\label{sec:AddingMatrices}

Two matrices, $\mathbb{A}$ and $\mathbb{B}$ can be summed if and only if the
matrices have equal rows and columns. To add $\mathbb{A}$ and
$\mathbb{B}$, sum the elements ${a}_{i,j}$ with ${b}_{i,j}$
such that:
\begin{align}
  \mathbb{Z} & = \mathbb{A} + \mathbb{B}
  \intertext{where}
  {z}_{i,j}  & = {a}_{i,j} + {b}_{i,j}
\end{align}
An example:
\begin{align}
  \mathbb{A}_{2,2} & = 
    \begin{bmatrix}
      1  &   5  \\
      3  &  -4  \\
    \end{bmatrix}
  \\
  \mathbb{B}_{2,2} & =
    \begin{bmatrix}
      4  &  -8  \\
      2  &  10  \\
    \end{bmatrix} 
  \\
  \mathbb{A} + \mathbb{B} & =
    \begin{bmatrix}
      1  &   5  \\
      3  &  -4  \\
    \end{bmatrix}
    +
    \begin{bmatrix}
      4  &  -8  \\
      2  &  10  \\
    \end{bmatrix}
  \\
  & =
    \begin{bmatrix}
      (1 + 4)  &  (5 + -8) \\
      (3 + 2)  & (-4 + 10) \\
    \end{bmatrix}
  \\
  & =
    \begin{bmatrix}
      5  & -3 \\
      5  &  6 \\
    \end{bmatrix}
\end{align}

\section{Subtracting Matrices}
\label{sec:SubtractingMatrices}
Two matrices, $\mathbb{A}$ and $\mathbb{B}$ can be subtracted if and only if the
matrices have matching rows and columns. To subtract $\mathbb{B}$ from
$\mathbb{A}$, subtract the elements ${b}_{i,j}$ from ${a}_{i,j}$
such that:
\begin{align}
  \mathbb{Z} & = \mathbb{A} - \mathbb{B}
  \intertext{where}
  {z}_{i,j}  & = {a}_{i,j} - {b}_{i,j}
\end{align}
An example:
\begin{align}
  \mathbb{A}_{2,2} & = 
    \begin{bmatrix}
      1  &   5  \\
      3  &  -4  \\
    \end{bmatrix}
  \\
  \mathbb{B}_{2,2} & =
    \begin{bmatrix}
      4  &  -8  \\
      2  &  10  \\
    \end{bmatrix} 
  \\
  \mathbb{A} - \mathbb{B} & =
    \begin{bmatrix}
      1  &   5  \\
      3  &  -4  \\
    \end{bmatrix}
    -
    \begin{bmatrix}
      4  &  -8  \\
      2  &  10  \\
    \end{bmatrix}
  \\
  & =
    \begin{bmatrix}
      (1 - 4)  &  (5 - -8) \\
      (3 - 2)  & (-4 - 10) \\
    \end{bmatrix}
  \\
  & =
    \begin{bmatrix}
      -3  &  13 \\
       1  & -14 \\
    \end{bmatrix}
\end{align}

\section{Multiplying Matrices}
\label{sec:MultiplyingMatrices}
Formal definitions for the multiplication of matrices like the one mentioned at
the start of this chapter appear scary, but really, it is just \emph{a lot}\footnote{
Because of this, expect to use up \emph{a lot more} paper.} of simple
additions.

\begin{enumerate}
\item The first rule for multiplying matrices is that the number of columns in
      $\mathbb{A}$ matches the number of rows in $\mathbb{B}$ (that is to say
      $\mathbb{A}_{N}$ = $\mathbb{B}_{M}$).
\item Remember that $\mathbb{Z}_{i,j}$ is a merely sum of several
      multiplication operations.
\end{enumerate}

Formally, $ {z}_{i,j} = \sum_{k=1}^{N} {a}_{i,k} \times {b}_{k,j} $

Broken down, this means for each element in a given row in
$\mathbb{A}$, we must multiply that element by a matching element in it's
corresponding column in $\mathbb{B}$ and sum all the results.

For the matrices $\mathbb{A}$, $\mathbb{B}$ and $\mathbb{P}$:
\begin{align}
\begin{bmatrix}
  ~\textcolor{neekiRed}{p_{1,1}}~&~p_{1,2}~ \\
  ~p_{2,1}~&~\textcolor{neekiGreen}{p_{2,2}}~
\end{bmatrix}
 &~=~ 
\begin{bmatrix}
  ~\textcolor{neekiRed}{a_{1,1}}~&~\textcolor{neekiRed}{a_{1,2}}~&~
                                              \textcolor{neekiRed}{a_{1,3}}~ \\
  ~\textcolor{neekiGreen}{a_{2,1}}~&~\textcolor{neekiGreen}{a_{2,2}}~&~
                                            \textcolor{neekiGreen}{a_{2,3}}~
\end{bmatrix}
~\times~
\begin{bmatrix}
  ~\textcolor{neekiRed}{b_{1,1}}~&~\textcolor{neekiGreen}{b_{1,2}}~ \\
  ~\textcolor{neekiRed}{b_{2,1}}~&~\textcolor{neekiGreen}{b_{2,2}}~ \\
  ~\textcolor{neekiRed}{b_{3,1}}~&~\textcolor{neekiGreen}{b_{3,2}}~
\end{bmatrix}
\intertext{where:}
\textcolor{neekiRed}{p_{1,1}}&\textcolor{neekiRed}{~=~a_{1,1}~\times~b_{1,1}~+~a_{1,2}~\times~b_{2,1}~+~a_{1,3}~\times~b_{3,1}} \nonumber \\
\textcolor{neekiPurple}{p_{2,1}}&~=\textcolor{neekiPurple}{~a_{2,1}~\times~b_{1,1}~+~a_{2,2}~\times~b_{2,1}~+~a_{2,3}~\times~b_{3,1}} \nonumber \\
\textcolor{neekiGreen}{p_{1,2}}&\textcolor{neekiGreen}{~=~a_{1,1}~\times~b_{1,2}~+~a_{1,2}~\times~b_{2,2}~+~a_{1,3}~\times~b_{3,2}} \nonumber \\
p_{2,2}&~=~a_{2,1}~\times~b_{1,2}~+~a_{2,2}~\times~b_{2,2}~+~a_{2,3}~\times~b_{3,2} \nonumber
\end{align}

\section{Identity Matrix}
\label{sec:IdentityMatrix}
For the identity matrix:
\begin{align}
  \mathbb{A}~\times~\mathbb{I}~&=~\mathbb{A} \nonumber
\end{align}

Proof by general case:
\begin{align}
  \begin{bmatrix}
    ~1~&~0~\\
    ~0~&~1~\\
  \end{bmatrix}
  \times
  \begin{bmatrix}
    ~a~&~b~\\
    ~c~&~d~\\
  \end{bmatrix}
  &=
  \begin{bmatrix}
    ~[(1~\times~a)+(0~\times~c)]~&~[(1~\times~b)+(0~\times~c)]~\\
    ~[(0~\times~a)+(1~\times~c)]~&~[(0~\times~b)+(1~\times~d)]~\\
  \end{bmatrix}\\
  &=
  \begin{bmatrix}
    ~a~&~b~\\
    ~c~&~d~\\
  \end{bmatrix}
\end{align}

This is the matrix equivalent of the number 1\footnote{NOTE: this is only
equivalent to one, it is not actually the value of one (to my knowledge).}

\begin{align}
  1~\times~a~&=~a \\
  \frac{1}{a}~\times~a~&=~1 \\
  \intertext{in matrix terms:}
  \mathbb{A}^{-1}~\times~\mathbb{A} = \mathbb{I}
\end{align}

This leads into the topic of calculating the inverse of a matrix.

\section{Inverse of Matrices}
\label{sec:InverseOfMatrices}
For a 2x2 matrix:
\begin{align}
  \mathbb{A} & = 
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \\
  \mathbb{A}^{-1} & =
    \frac{1}{ad-bc} \times
    \begin{bmatrix}
      &~d~&~-b~& \\
      &~-c~&~a~&
    \end{bmatrix}  
\end{align}

For a 3x3 matrix:
%\begin{align}
%
%\end{align}
TODO THIS\ldots

For a 4x4 matrix, in the words of Sal Kahn, ``You'll be there all day, and for a
5x5 you're almost bound to make a mistake and is best left for a
computer.''\footnote{``In my mind, the only thing less pleasant than inverting
a 3 by 3 matrix is inverting a 4 by 4 matrix.'' -- both quotes from
http://www.khanacademy.org/video/inverting-matrices--part-2?playlist=Linear\%20Algebra}

\section{TODO: Dividing Matrices}
\label{sec:DividingMatrices}

\section{Summary of Matrix Operations}
\label{sec:SummaryOfMatrixOperations}
